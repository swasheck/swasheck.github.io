<!doctype html>
<html lang="en"><head>
    <title>Finding Value in Incremental Statistics, Pt. 2</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="" />

    
    
    
    <link rel="stylesheet" href="../../css/theme.min.css">

    
    
    

    
</head>
<body>
        <div id="content" class="mx-auto"><header class="container mt-sm-5 mt-4 mb-4 mt-xs-1">
    <div class="row">
        <div class="col-sm-4 col-12 text-sm-right text-center pt-sm-4">
            <a href="../../" class="text-decoration-none">
                <img id="home-image" class="rounded-circle"
                    
                        src="../../images/avatar.png"
                    
                />
            </a>
        </div>
        <div class="col-sm-8 col-12 text-sm-left text-center">
            <h2 class="m-0 mb-2 mt-4">
                <a href="../../" class="text-decoration-none">
                    
                        stochasmos
                    
                </a>
            </h2>
            <p class="text-muted mb-1">
                
                    thoughts
                
            </p>
            <ul id="nav-links" class="list-inline mb-2">
                
                
                    <li class="list-inline-item">
                        <a class="badge badge-white " href="../../about/" title="about">about</a>
                    </li>
                
                    <li class="list-inline-item">
                        <a class="badge badge-white " href="../../post/" title="posts">posts</a>
                    </li>
                
                    <li class="list-inline-item">
                        <a class="badge badge-white " href="../../categories/" title="categories">categories</a>
                    </li>
                
            </ul>
            <ul id="nav-social" class="list-inline">
                
                    <li class="list-inline-item mr-3">
                        <a href="http://github.com/swasheck" target="_blank">
                            <i class="fab fa-github fa-1x text-muted"></i>
                        </a>
                    </li>
                
                    <li class="list-inline-item mr-3">
                        <a href="https://www.linkedin.com/in/swasheck" target="_blank">
                            <i class="fab fa-linkedin-in fa-1x text-muted"></i>
                        </a>
                    </li>
                
            </ul>
        </div>
    </div>
    <hr />
</header>
<div class="container">
    <div class="pl-sm-2">
        <div class="mb-3">
            <h3 class="mb-0">Finding Value in Incremental Statistics, Pt. 2</h3>
            
            <small class="text-muted">Published 2015-11-03</small>
        </div>

        <article>
            <p><a href="https://swasheck.wordpress.com/2015/10/21/finding-value-in-incremental-statistics-pt-1/">Previously</a>, I did a little investigation into incremental statistics and what benefit they can provide. The goal was to find the intersection of update timings for incremental and non-incremental statistics, with the hope of being able to demonstrate a reclamation of statistics maintenance times - and perhaps higher sample rates. Summary: we can, and <code>auto_stats</code> is a beneficiary. However, is this a solution in search of a problem? If you&rsquo;ve been bumping into statistics maintenance issues, you&rsquo;re probably already familiar with more of the hands-on intervention available (my personal favorite is <a href="https://ola.hallengren.com/">Ola Hallengren&rsquo;s solution</a>).</p>
<!-- raw HTML omitted -->
<p>Since we already know that the optimizer <a href="http://sqlperformance.com/2015/05/sql-statistics/incremental-statistics-are-not-used-by-the-query-optimizer">does not use statistics at the partition level</a>, maybe we can parlay the potentially-higher sample rates into better plans with faster query execution times, or at least better resource utilization.</p>
<p>All tests were evaluated against a 1 percent sample rate non-incremental statistic on the large <code>PostHistory</code> table, using <a href="http://www.sqlsentry.com/products/plan-explorer/sql-server-query-view">SQL Sentry Plan Explorer</a> to capture relevant metrics. The query in question is a simple data pull across two partitions, with no aggregates.</p>
<p>[code lang=text]
select
PostCreationDate = p.CreationDate, 
PostOwner = pu.DisplayName,
p.Title, 
pt.PostType,
PostHistoryUser = phu.DisplayName,
PostHistoryDate = ph.CreationDate,
Comment
from PostHistory ph
join Posts p
on ph.PostId = p.id
and ph.SiteId = p.SiteId
join Sites s 
on ph.SiteId = s.SiteId
join PostTypes pt
on p.PostTypeId = pt.PostTypeId
join Users phu
on phu.Id = ph.UserId
and phu.SiteId = ph.SiteId
join Users pu
on pu.Id = p.OwnerUserId
and pu.SiteId = p.SiteId
where s.Address = &lsquo;dba.stackexchange.com&rsquo;
and ph.CreationDate &gt;= &lsquo;2014-06-30&rsquo;
and ph.CreationDate &lt; &lsquo;2015-10-01&rsquo;
order by p.CreationDate desc, ph.CreationDate desc
option (recompile);
[/code]</p>
<p>As a baseline, we see that the 1 percent, non-incremental index takes the following plan shape:</p>
<p><img src="https://swasheck.files.wordpress.com/2015/11/non-incremental-plan-diagram.png" alt="non-incremental plan diagram.png"></p>
<p>with these estimates/actuals (a common measure of stats validity):</p>
<p><img src="https://swasheck.files.wordpress.com/2015/11/final-est-act.png" alt="non-incremental est. stats.png"></p>
<p>Finally, the information in the MemoryGrantInfo node of the plan XML shows the following:</p>
<p>[code lang=text]
<!-- raw HTML omitted -->
[/code]</p>
<p>Based on what we see here, we&rsquo;ve got quite a discrepancy between estimated and actual row counts on both the final product and the actual object that we&rsquo;re evaluating.</p>
<p><img src="https://swasheck.files.wordpress.com/2015/11/non-incremental-est-stats.png" alt="non-incremental est. stats.png"></p>
<p>Our memory grant information shows us that if we&rsquo;d chosen a serial plan, we&rsquo;d have a required amount of 4 MB grant (internal structures required to begin the query). However, we can see that we got a parallel plan, which asked for a 323 MB grant. Of that amount, ~17 MB was &ldquo;required&rdquo; and we did spike to 24 MB of used memory in this query. So even though the we overestimated our grant request, it worked out for us because we got around a potential spill. A key part of generating these estimates is how the cardinality estimation process as that will influence the decisions made, not just for planning, but also for memory grant requests. &ldquo;Additional memory&rdquo; is a portion of the memory grant that is calculated based off of cardinality and row sze of the temporary set of rows in memory. An overestimate on the grant can lead to memory waste. An underestimate on the grant can lead to spills. In this case, we&rsquo;ve overestimated and taken a grant that is nearly 300 MB more than we needed.</p>
<p>Does incremental stats help with this? At a 10 percent sample rate we lose the Key Lookup on the Users table, satisfying the request for the display name of the user who owns the original post with the previous seek on the CI/primary key. Interestingly, the second key lookup remains as it satisfies the request for the display name of a user who modified the original post in any way. Interestingly, though the plan shape changed, the estimates didn&rsquo;t. Additionally, the overall CPU times and query times showed no discernable pattern of improvement as we moved from 1 percent to 10 percent. However, there was improvement in the memory grant and in reads. Both of these are explicable by the absence of the first key lookup on the Users table. Specifically, the difference in Required Memory can lead us to the removal of that operator as the explanation. SQL Server requires 512KB (the difference between the 1 percent and 1o percent SerialRequiredMemory) to set up an operator, and the requirement for a parallel query would be DOP * 512KB, or 2048KB which is the difference between the plans' RequiredMemory attributes.</p>
<p>[code lang=text]
Memory grant for incremental statistic sampled at 10 pct.</p>
<p><!-- raw HTML omitted -->
[/code]</p>
<p>Curiously, at a 30 percent sample rate, the plan takes the original shape with the large memory grant and extra key lookup operator. The memory grant information remains the same for all sample rates, up to <code>FULLSCAN</code>. However, an analysis of <code>PostHistory.CreationDate</code> shows skewed data. A future test could be to analyze such a test on data that is more evenly distributed.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The results of this investigation should not be interpreted to mean that incremental statistics do not lead to performance gain on queries, because the investigation was so narrow. However, if my suspicion that statistics skew is the reason behind the consistently suboptimal plan generation, then at a certain row count the skew would be significant enough to cause poor cardinality estimate resolution regardless of sample rate. In this regard, incremental statistics on large partitioned tables, the ostensible target implementation, would provide no tangible performance benefit for plan generation.</p>
<p>Therefore, at this point, it seems the most conclusive thing that we can say about incremental statistics is that, based on my previous post, we can use incremental statistics to reclaim maintenance time and, perhaps, time on <code>auto_update</code> events.</p>

        </article>
    </div>

    

            </div>
        </div><footer class="text-center pb-1">
    <small class="text-muted">
        
            &copy; 2022, swasheck
        
        <br>
        Powered by <a href="https://gohugo.io/" target="_blank">Hugo</a>
        and <a href="https://github.com/austingebauer/devise" target="_blank">Devise</a>
    </small>
</footer>
</body>
</html>
